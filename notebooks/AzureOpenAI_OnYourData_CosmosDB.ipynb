{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efef693-3a87-427b-b357-72c1447f34ac",
   "metadata": {},
   "source": [
    "# Microsoft Azure OpenAI On Your Data with CosmosDB\n",
    "\n",
    "In this notebook we'll use CosmosDB indices to provide grounding data for queries to Azure OpenAI models using the Azure OpenAI On Your Data service.\n",
    "\n",
    "The Azure OpenAI On Your Data service currently supports three search scenarios for retrieval of documents that will be sent to the LLM for processing:\n",
    "\n",
    "1) vector search using embeddings generated using Azure OpenAI (text-embeddigns-v3).\n",
    "2) vector search embedding your own pdf files.\n",
    "\n",
    "Each of these examples will be covered in the following sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95901137-91f3-40aa-bf99-bb25e0a3a11e",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "For this example, you will need:\n",
    "* Python 3.11 or later\n",
    "* An Azure OpenAI Resource\n",
    "    * One multimodal model (gpt-4o-mini) should be deployed for your resource to enable chatting about your data and allow images and audios.\n",
    "    * For vector search this notebook uses the Azure OpenAI text-embedding-3-small model. The examples below will assume you are using the model `text-embedding-3-small`, but can be updated to suit your needs.\n",
    "* The [OpenAI Python Client](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
    "\n",
    "### Create and Configurate CosmosDB \n",
    "\n",
    "If you don't have a CosmosDB cluster, you can read more about how to get started here in the official [https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-portal](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-portal).\n",
    "\n",
    "\n",
    "### Configure Azure OpenAI Resource\n",
    "\n",
    "If you don't have an Azure OpenAI resource, detailed information about how to obtain one can be found in the [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart?tabs=command-line&pivots=programming-language-python) for the Azure OpenAI On Your Data service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9629b3-e18f-4f6b-a819-43416d4f3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv pymongo openai pymupdf langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46748ec1-5896-4bdf-9be1-f0be0dfdeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "from cosmosdb_rag_attachment import Document, CosmosDBRAG, ChatClient, PDFChunk\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad67824-eef2-4458-8c02-abe8458a1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parameters from .env file\n",
    "CONNECTION_STRING = config['AZURE_COSMOSDB_MONGO_VCORE_CONNECTION_STRING']\n",
    "DATABASE_NAME = config['AZURE_COSMOSDB_MONGO_VCORE_DATABASE']\n",
    "COLLECTION_NAME = config['AZURE_COSMOSDB_MONGO_VCORE_CONTAINER']\n",
    "AZURE_OPENAI_ENDPOINT = config['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_KEY = config['AZURE_OPENAI_KEY']\n",
    "AZURE_OPENAI_PREVIEW_API_VERSION = config['AZURE_OPENAI_PREVIEW_API_VERSION']\n",
    "AZURE_OPENAI_MODEL = config['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_EMBEDDING_NAME = config['AZURE_OPENAI_EMBEDDING_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d126770-4de7-4707-b6b3-a5581572435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paramerters for MongoDB Vector index\n",
    "index_name = \"vector_search_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5907a520-9ea3-43f4-9ef1-2beaf9b21093",
   "metadata": {},
   "source": [
    "# PDF Upload Instructions for RAG Application\n",
    "\n",
    "Welcome to the **Retrieval-Augmented Generation (RAG)** application!\n",
    "\n",
    "In this notebook, we are demonstrating how to upload and process a PDF file using the RAG approach. Specifically, this notebook was tested using the book **_Speech and Language Processing_** by **Daniel Jurafsky** and **James H. Martin**. Follow the instructions below to upload your own PDF file and get started.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Download the Book**:\n",
    "   - If you don't have the book already, please download [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3bookaug20_2024.pdf) by Jurafsky and Martin. This is the book that this notebook was tested with.\n",
    "   \n",
    "2. **Upload the PDF**:\n",
    "   - Specify your file location in the following cell to upload the PDF file of the book (or any other PDF you'd like to process).\n",
    "   \n",
    "3. **Processing the PDF**:\n",
    "   - Once the PDF is uploaded, the notebook will process the file and allow you to interact with it using the RAG methodology.\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "- This notebook was **tested with the book \"Speech and Language Processing\"** by **Jurafsky and Martin**, which is an excellent resource for learning about natural language processing (NLP).\n",
    "- If you'd like to use this notebook with other PDFs, it should work just as well with any text-based PDF. However, if your document is not text-based (i.e., itâ€™s scanned or image-based), it may not be supported yet.\n",
    "\n",
    "---\n",
    "\n",
    "### Example PDF File:\n",
    "- **Book Title**: _Speech and Language Processing_\n",
    "- **Authors**: Daniel Jurafsky and James H. Martin\n",
    "- **Tested Version**: 3rd edition\n",
    "\n",
    "\n",
    "Happy exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82685584-e727-4a7a-b518-d017cb2f5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pdf_file = \"/Users/pabloracana/Downloads/ed3bookaug20_2024.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1efebf-a529-434d-b37d-372a8c784aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pabloracana/Desktop/Projects/sample-app-aoai-chatGPT/notebooks/cosmosdb_rag_attachment.py:33: UserWarning: You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb\n",
      "  self.client = MongoClient(connection_string)\n"
     ]
    }
   ],
   "source": [
    "# Initialize clients\n",
    "db_client = CosmosDBRAG(CONNECTION_STRING, DATABASE_NAME, COLLECTION_NAME)\n",
    "ai_client = ChatClient(AZURE_OPENAI_ENDPOINT, \n",
    "                       AZURE_OPENAI_KEY, \n",
    "                       AZURE_OPENAI_PREVIEW_API_VERSION, \n",
    "                       AZURE_OPENAI_MODEL, \n",
    "                       AZURE_OPENAI_EMBEDDING_NAME)\n",
    "pdf_chunker = PDFChunk(user_pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27eadf44-8be8-451a-8226-e2226c2f22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_application(db_client, ai_client, pdf_chunker):\n",
    "    print(\"First, we create the Index to optimize the search\")\n",
    "    db_client.create_or_update_vector_index(index_name, 'vector_content')\n",
    "    \n",
    "    if db_client.verify_file_exists:\n",
    "        print(\"PDF File already exist in the Database, you can continue with the Chat\")\n",
    "    else:\n",
    "        print(\"Reading PDF file and generating chunks\")\n",
    "        documents = pdf_chunker.extract_and_chunk_pdf(chunk_size=520, chunk_overlap=20)\n",
    "        print(\"Generating vector representations and storing in the DB\")\n",
    "        for doc in tqdm(documents):\n",
    "            doc.vector_content = ai_client.generate_embeddings(doc.content)\n",
    "            doc._id = f\"doc:{uuid.uuid4()}\"\n",
    "            db_client.create_document(doc)\n",
    "\n",
    "    print(\"All set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8f7021-660d-480c-b2f1-8a4e643e40e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we create the Index to optimize the search\n",
      "Index already exists\n",
      "PDF File already exist in the Database, you can continue with the Chat\n",
      "All set!\n"
     ]
    }
   ],
   "source": [
    "rag_application(db_client, ai_client, pdf_chunker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ca07006-6456-4066-97e4-2af119d299c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_rag(user_question):\n",
    "    user_embedding = ai_client.generate_embeddings(user_question)\n",
    "    retrieved_docs = db_client.similarity_search(user_embedding, k=3)\n",
    "    docs_content = [doc['content'] for doc in retrieved_docs]\n",
    "    response = ai_client.chat_completion(user_prompt=user_question,\n",
    "                                         documents=docs_content)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67f6f953-63a3-4d5c-8553-970cfe1bb211",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Why it's import to implement retrieval augmented generation when creating question-answering applications?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd011831-fd9c-4450-90b6-ce505d4e6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = chat_completion_rag(user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba7fd4fe-62fc-4fe3-adbb-4d16f8dd2194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing retrieval-augmented generation (RAG) in question-answering applications is important because it addresses several limitations of simple question-answering methods. While large language models can generate answers based on their pretraining, they often struggle with issues like hallucination, lack of supporting textual evidence, and inability to answer questions based on specific proprietary data. \n",
      "\n",
      "RAG improves the QA process by first retrieving relevant text passages and then conditioning the language model's output on these passages, providing more accurate and contextually grounded responses. Essentially, it allows the model to generate answers with real textual evidence, making the application more reliable and effective in delivering accurate information. This approach also overcomes the limitations of generating answers solely from pre-trained knowledge. \n",
      "\n",
      "You can find more about this on pages 309-311.\n"
     ]
    }
   ],
   "source": [
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdd533-53ca-4443-86d3-b4542d3ee079",
   "metadata": {},
   "source": [
    "# Next Steps for RAG Application\n",
    "\n",
    "## 1. **Modify Embedding Ingestion Method**\n",
    "   - **Current Issue:** Unable to process in batches due to rate limits on the free tier.\n",
    "   - **Next Steps:**\n",
    "     - Implement batch ingestion by segmenting data into smaller chunks.\n",
    "     - Add rate limiting logic to handle free tier constraints effectively.\n",
    "\n",
    "## 2. **Update Vector Search Method**\n",
    "   - **Current Issue:** Unable to use HNSW (Hierarchical Navigable Small World) due to free tier cluster size limitations.\n",
    "\n",
    "## 3. **Validate Outputs**\n",
    "   - **Next Steps:**\n",
    "     - Validate results against expected outcomes for various queries.\n",
    "     - Tune parameters, such as:\n",
    "       - Number of retrieved documents per query.\n",
    "       - Embedding dimensionality.\n",
    "       - Similarity thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27852674-a9dc-41da-a66f-79407a641251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oz-assessment",
   "language": "python",
   "name": "oz-assessment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
